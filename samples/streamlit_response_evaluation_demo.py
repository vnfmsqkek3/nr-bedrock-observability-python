#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlitì—ì„œ ëª¨ë¸ ë§Œì¡±ë„ í‰ê°€ ë°ëª¨

ì´ ìƒ˜í”Œì€ Streamlit UIë¥¼ ì‚¬ìš©í•˜ì—¬ AWS Bedrock ëª¨ë¸ì˜ ì‘ë‹µì— ëŒ€í•œ
ë§Œì¡±ë„ í‰ê°€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•: streamlit run streamlit_response_evaluation_demo.py
"""

import os
import json
import uuid
import boto3
import time
import logging
import streamlit as st

# LangChain ì„í¬íŠ¸ (ì„ íƒ ì‚¬í•­)
try:
    import langchain
    import langchain.llms
    from langchain.memory import ConversationBufferMemory
    from langchain.chains import ConversationalRetrievalChain
    LANGCHAIN_AVAILABLE = True
    LANGCHAIN_VERSION = langchain.__version__
except ImportError:
    LANGCHAIN_AVAILABLE = False
    LANGCHAIN_VERSION = None

from nr_bedrock_observability import (
    monitor_bedrock,
    create_response_evaluation_collector,
    create_streamlit_response_evaluation_collector
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Streamlit ì•± ì„¤ì •
st.set_page_config(
    page_title="Bedrock ëª¨ë¸ ë§Œì¡±ë„ í‰ê°€ ë°ëª¨",
    page_icon="ğŸ¤–",
    layout="wide"
)

def get_bedrock_client():
    """
    AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    """
    try:
        return boto3.client('bedrock-runtime')
    except Exception as e:
        st.error(f"Bedrock í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_bedrock_kb_client():
    """
    AWS Bedrock ì§€ì‹ ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    """
    try:
        return boto3.client('bedrock-agent-runtime')
    except Exception as e:
        logger.warning(f"Bedrock ì§€ì‹ ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_monitored_client(bedrock_client, application_name, nr_api_key, trace_id):
    """
    New Relic ëª¨ë‹ˆí„°ë§ì´ ì ìš©ëœ Bedrock í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    """
    try:
        return monitor_bedrock(bedrock_client, {
            'application_name': application_name,
            'new_relic_api_key': nr_api_key,
            'trace_id': trace_id
        })
    except Exception as e:
        st.error(f"ëª¨ë‹ˆí„°ë§ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def invoke_model(client, model_id, prompt, kb_id=None):
    """
    Bedrock ëª¨ë¸ í˜¸ì¶œ (ì§€ì‹ ê¸°ë°˜ ì‚¬ìš© ì˜µì…˜ í¬í•¨)
    """
    try:
        start_time = time.time()
        
        if kb_id:
            # ì§€ì‹ ê¸°ë°˜ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
            kb_client = get_bedrock_kb_client()
            if not kb_client:
                st.warning("ì§€ì‹ ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ì–´ ì¼ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return invoke_model(client, model_id, prompt)
                
            response = kb_client.retrieve_and_generate(
                knowledgeBaseId=kb_id,
                input={
                    "text": prompt
                },
                retrieveAndGenerateConfiguration={
                    "type": "KNOWLEDGE_BASE",
                    "knowledgeBaseConfiguration": {
                        "modelId": model_id,
                        "generationConfiguration": {
                            "temperature": 0.5,
                            "maxTokens": 1000
                        }
                    }
                }
            )
            
            end_time = time.time()
            response_time = int((end_time - start_time) * 1000)
            
            return {
                "text": response["output"]["text"], 
                "response_time": response_time,
                "kb_used": True
            }
        else:
            # ì¼ë°˜ ëª¨ë¸ í˜¸ì¶œ
            response = client.invoke_model(
                modelId=model_id,
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 500,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ]
                        }
                    ]
                })
            )
            
            end_time = time.time()
            response_time = int((end_time - start_time) * 1000)
            
            response_body = json.loads(response['body'].read().decode('utf-8'))
            return {
                "text": response_body['content'][0]['text'],
                "response_time": response_time,
                "kb_used": False,
                "token_info": {
                    "input_tokens": response_body.get("usage", {}).get("input_tokens", 0),
                    "output_tokens": response_body.get("usage", {}).get("output_tokens", 0),
                    "total_tokens": response_body.get("usage", {}).get("total_tokens", 0)
                }
            }
    except Exception as e:
        st.error(f"ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def invoke_with_langchain(client, model_id, prompt):
    """
    LangChainì„ ì‚¬ìš©í•˜ì—¬ Bedrock ëª¨ë¸ í˜¸ì¶œ
    """
    if not LANGCHAIN_AVAILABLE:
        st.error("LangChainì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    try:
        from langchain.llms import Bedrock
        
        start_time = time.time()
        
        # LangChain Bedrock LLM ì„¤ì •
        llm = Bedrock(
            model_id=model_id,
            client=client
        )
        
        # ë‹¨ìˆœ ìƒì„± ìš”ì²­
        response = llm.predict(prompt)
        
        end_time = time.time()
        response_time = int((end_time - start_time) * 1000)
        
        # ì…ë ¥ê³¼ ì¶œë ¥ í† í° ìˆ˜ ì˜ˆì¸¡ (ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
        input_tokens = len(prompt.split()) * 1.3  # ëŒ€ëµì ì¸ ì¶”ì •ê°’
        output_tokens = len(response.split()) * 1.3  # ëŒ€ëµì ì¸ ì¶”ì •ê°’
        
        return {
            "text": response,
            "response_time": response_time,
            "kb_used": False,
            "langchain_used": True,
            "langchain_version": LANGCHAIN_VERSION,
            "langchain_chain_type": "SimpleLLMChain",
            "token_info": {
                "input_tokens": int(input_tokens),
                "output_tokens": int(output_tokens),
                "total_tokens": int(input_tokens + output_tokens)
            }
        }
    except Exception as e:
        st.error(f"LangChain ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def main():
    # ì•± ì œëª©
    st.title("AWS Bedrock ëª¨ë¸ ë§Œì¡±ë„ í‰ê°€ ë°ëª¨")
    st.subheader("ëª¨ë¸ ì‘ë‹µ ë§Œì¡±ë„ í‰ê°€ ì˜ˆì œ")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("ì„¤ì •")
        
        # New Relic API í‚¤ ì„¤ì •
        nr_api_key = st.text_input(
            "New Relic API í‚¤",
            value=os.environ.get('NEW_RELIC_API_KEY', 'test-api-key'),
            type="password"
        )
        
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„ ì„¤ì •
        application_name = st.text_input(
            "ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„",
            value="Streamlit-Model-Evaluation-Demo"
        )
        
        # ëª¨ë¸ ì„ íƒ
        model_id = st.selectbox(
            "ì‚¬ìš©í•  ëª¨ë¸",
            [
                "anthropic.claude-3-sonnet-20240229-v1:0",
                "anthropic.claude-3-haiku-20240307-v1:0",
                "anthropic.claude-3-opus-20240229-v1:0",
                "anthropic.claude-3-5-sonnet-20240620-v1:0",
                "anthropic.claude-3-5-sonnet-20241022-v2:0"
            ]
        )
        
        # í†µí•© ì˜µì…˜
        st.subheader("í†µí•© ì˜µì…˜")
        
        # Bedrock ì§€ì‹ ê¸°ë°˜ ì˜µì…˜
        use_kb = st.checkbox("Bedrock ì§€ì‹ ê¸°ë°˜ ì‚¬ìš©", value=False)
        
        kb_id = None
        kb_name = None
        
        if use_kb:
            kb_id = st.text_input("ì§€ì‹ ê¸°ë°˜ ID")
            kb_name = st.text_input("ì§€ì‹ ê¸°ë°˜ ì´ë¦„ (ì„ íƒì‚¬í•­)")
            
        # LangChain ì˜µì…˜
        use_langchain = st.checkbox("LangChain ì‚¬ìš©", value=False, disabled=not LANGCHAIN_AVAILABLE)
        
        if not LANGCHAIN_AVAILABLE and use_langchain:
            st.warning("LangChain íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ì´ˆê¸°í™”
        if st.button("ì„¸ì…˜ ì´ˆê¸°í™”"):
            st.session_state.clear()
            st.experimental_rerun()
        
        # ëª¨ë¸ ì„±ëŠ¥ í†µê³„
        st.subheader("ëª¨ë¸ í‰ê°€ í†µê³„")
        if "evaluation_stats" not in st.session_state:
            st.session_state.evaluation_stats = {}
        
        for model in ["anthropic.claude-3-sonnet", "anthropic.claude-3-haiku", "anthropic.claude-3-opus"]:
            if model in st.session_state.evaluation_stats:
                stats = st.session_state.evaluation_stats[model]
                st.metric(
                    label=f"{model}",
                    value=f"{stats['overall_avg']:.1f}/10",
                    delta=f"ì •í™•ì„±: {stats['accuracy_avg']:.1f}"
                )
    
    # íŠ¸ë ˆì´ìŠ¤ ID ì´ˆê¸°í™” (ì„¸ì…˜ ì‹œì‘ì‹œ)
    if "trace_id" not in st.session_state:
        st.session_state.trace_id = str(uuid.uuid4())
    
    # í‰ê°€ ìœ í˜• ì„ íƒ (ê¸°ë³¸ ë˜ëŠ” ìƒì„¸)
    evaluation_type = st.radio(
        "í‰ê°€ ìœ í˜• ì„ íƒ",
        ["ê¸°ë³¸ í‰ê°€", "ìƒì„¸ í‰ê°€"],
        horizontal=True
    )
    
    # ì¿¼ë¦¬ ìœ í˜• ì„ íƒ
    query_types = ["ì¼ë°˜ ì§€ì‹", "ì°½ì˜ì  ìƒì„±", "ì½”ë”©/ê¸°ìˆ ", "ë¶„ì„/ì¶”ë¡ "]
    selected_type = st.radio("ì§ˆë¬¸ ìœ í˜•", query_types, horizontal=True)
    
    # ë„ë©”ì¸ ì„ íƒ
    domain_types = ["ì¼ë°˜", "ê¸°ìˆ ", "ê³¼í•™", "ë¹„ì¦ˆë‹ˆìŠ¤", "ì˜ˆìˆ ", "ê¸°íƒ€"]
    selected_domain = st.selectbox("ë„ë©”ì¸ ë¶„ì•¼", domain_types)
    
    # ì¿¼ë¦¬ ìœ í˜•ë³„ ì˜ˆì œ í”„ë¡¬í”„íŠ¸
    example_prompts = {
        "ì¼ë°˜ ì§€ì‹": "ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ì™€ ë°œì „ ê³¼ì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì°½ì˜ì  ìƒì„±": "ì•ˆê°œê°€ ììš±í•œ ê³ ì„±ì´ ë°°ê²½ì¸ íŒíƒ€ì§€ ì†Œì„¤ì˜ ì²« ë¬¸ë‹¨ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
        "ì½”ë”©/ê¸°ìˆ ": "Reactì—ì„œ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ë“¤ê³¼ ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¶„ì„/ì¶”ë¡ ": "ê¸°í›„ ë³€í™”ê°€ ê¸€ë¡œë²Œ ì‹ëŸ‰ ì•ˆë³´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ê°€ëŠ¥í•œ í•´ê²°ì±…ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
    }
    
    # í”„ë¡¬í”„íŠ¸ ì…ë ¥ í¼
    with st.form("prompt_form"):
        prompt = st.text_area(
            "Bedrockì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            value=example_prompts[selected_type],
            height=100
        )
        submit_button = st.form_submit_button("ì‘ë‹µ ìƒì„±")
    
    # í¼ ì œì¶œ ì²˜ë¦¬
    if submit_button:
        # ì„¸ì…˜ì— ìƒˆ ì‘ë‹µ ìƒì„± í‘œì‹œ
        st.session_state.new_response = True
        st.session_state.current_model_id = model_id
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        bedrock_client = get_bedrock_client()
        if not bedrock_client:
            return
        
        # ëª¨ë‹ˆí„°ë§ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        monitored_client = get_monitored_client(
            bedrock_client,
            application_name,
            nr_api_key,
            st.session_state.trace_id
        )
        if not monitored_client:
            return
        
        # ë¡œë”© ìŠ¤í”¼ë„ˆ í‘œì‹œ
        with st.spinner("Bedrockì—ì„œ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # ì‹¤í–‰ ë°©ë²• ì„ íƒ
            if use_langchain and LANGCHAIN_AVAILABLE:
                result_data = invoke_with_langchain(monitored_client, model_id, prompt)
            elif use_kb and kb_id:
                result_data = invoke_model(monitored_client, model_id, prompt, kb_id=kb_id)
            else:
                result_data = invoke_model(monitored_client, model_id, prompt)
        
        if result_data:
            # ì‘ë‹µ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
            st.session_state.result = result_data["text"]
            st.session_state.response_time = result_data.get("response_time", 0)
            st.session_state.kb_used = result_data.get("kb_used", False)
            st.session_state.kb_id = kb_id if use_kb else None
            st.session_state.kb_name = kb_name if use_kb else None
            
            # LangChain ë©”íƒ€ë°ì´í„°
            st.session_state.langchain_used = result_data.get("langchain_used", False)
            st.session_state.langchain_version = result_data.get("langchain_version", LANGCHAIN_VERSION)
            st.session_state.langchain_chain_type = result_data.get("langchain_chain_type", None)
            
            # í† í° ì •ë³´
            token_info = result_data.get("token_info", {})
            st.session_state.prompt_tokens = token_info.get("input_tokens", 0)
            st.session_state.completion_tokens = token_info.get("output_tokens", 0)
            st.session_state.total_tokens = token_info.get("total_tokens", 0)
            
            # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            st.session_state.query_type = selected_type
            st.session_state.domain = selected_domain
            
            # ì„±ê³µ ë©”ì‹œì§€
            st.success("ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # ì‘ë‹µ í‘œì‹œ
    if "result" in st.session_state:
        st.subheader("Bedrock ì‘ë‹µ")
        
        # ëª¨ë¸ ID ì •ë³´
        model_name = st.session_state.current_model_id.split(':')[0]
        st.info(f"ëª¨ë¸: {model_name}")
        
        # ì‘ë‹µ ë‚´ìš©
        st.markdown(st.session_state.result)
        
        # ì‘ë‹µ ë©”íƒ€ë°ì´í„° í‘œì‹œ
        metadata_cols = st.columns(4)
        with metadata_cols[0]:
            st.metric("ì‘ë‹µ ì‹œê°„", f"{st.session_state.response_time}ms")
        
        with metadata_cols[1]:
            total_tokens = st.session_state.get("total_tokens", 0)
            st.metric("ì´ í† í° ìˆ˜", f"{total_tokens}")
        
        with metadata_cols[2]:
            kb_used = "ì‚¬ìš©í•¨" if st.session_state.get("kb_used", False) else "ì‚¬ìš© ì•ˆí•¨"
            st.metric("ì§€ì‹ ê¸°ë°˜", kb_used)
            
        with metadata_cols[3]:
            langchain_used = "ì‚¬ìš©í•¨" if st.session_state.get("langchain_used", False) else "ì‚¬ìš© ì•ˆí•¨"
            st.metric("LangChain", langchain_used)
        
        # êµ¬ë¶„ì„ 
        st.divider()
        
        # ì‘ë‹µ í‰ê°€ ìˆ˜ì§‘ê¸° ìƒì„±
        if "response_evaluation_collector" not in st.session_state:
            st.session_state.response_evaluation_collector = create_response_evaluation_collector(
                application_name=application_name,
                trace_id=st.session_state.trace_id
            )
        
        # Streamlit í‰ê°€ ìˆ˜ì§‘ê¸° ìƒì„±
        if "streamlit_evaluation_collector" not in st.session_state:
            st.session_state.streamlit_evaluation_collector = create_streamlit_response_evaluation_collector(
                application_name=application_name,
                response_evaluation_collector=st.session_state.response_evaluation_collector
            )
        
        # ìƒˆ ì‘ë‹µì´ ìƒì„±ëœ ê²½ìš° í‰ê°€ ìƒíƒœ ì´ˆê¸°í™”
        if st.session_state.get("new_response", False):
            st.session_state.streamlit_evaluation_collector.reset_evaluation()
            st.session_state.new_response = False
        
        # ì„ íƒí•œ í‰ê°€ ìœ í˜•ì— ë”°ë¼ UI ë Œë”ë§
        if evaluation_type == "ê¸°ë³¸ í‰ê°€":
            evaluation_data = st.session_state.streamlit_evaluation_collector.render_basic_evaluation_ui(
                model_id=st.session_state.current_model_id
            )
        else:
            evaluation_data = st.session_state.streamlit_evaluation_collector.render_detailed_evaluation_ui(
                model_id=st.session_state.current_model_id
            )
        
        # í‰ê°€ê°€ ì œì¶œë˜ë©´ ë°ì´í„° í‘œì‹œ ë° í†µê³„ ì—…ë°ì´íŠ¸
        if evaluation_data:
            # ëª¨ë¸ í‰ê°€ì— ì¶”ê°€ ì •ë³´ ì¶”ê°€
            if "response_evaluation_collector" in st.session_state:
                collector = st.session_state.response_evaluation_collector
                
                # ì¶”ê°€ ëª¨ë¸ ë©”íƒ€ë°ì´í„°
                model_provider = evaluation_data["model_id"].split('.')[0] if '.' in evaluation_data["model_id"] else None
                
                # ì§€ì‹ ê¸°ë°˜ ë° LangChain ì •ë³´
                kb_data = {
                    "kb_id": st.session_state.get("kb_id"),
                    "kb_name": st.session_state.get("kb_name"),
                    "kb_used_in_query": st.session_state.get("kb_used", False)
                }
                
                langchain_data = {
                    "langchain_used": st.session_state.get("langchain_used", False),
                    "langchain_version": st.session_state.get("langchain_version"),
                    "langchain_chain_type": st.session_state.get("langchain_chain_type")
                }
                
                token_data = {
                    "total_tokens": st.session_state.get("total_tokens", 0),
                    "prompt_tokens": st.session_state.get("prompt_tokens", 0),
                    "completion_tokens": st.session_state.get("completion_tokens", 0)
                }
                
                # ì‘ë‹µ ì‹œê°„
                response_time_data = {
                    "response_time_ms": st.session_state.get("response_time", 0)
                }
                
                # ìƒˆë¡œìš´ í‰ê°€ ê¸°ë¡ ìƒì„±
                try:
                    augmented_eval = collector.record_evaluation(
                        model_id=evaluation_data["model_id"],
                        model_provider=model_provider,
                        overall_score=evaluation_data["overall_score"],
                        relevance_score=evaluation_data.get("relevance_score"),
                        accuracy_score=evaluation_data.get("accuracy_score"),
                        completeness_score=evaluation_data.get("completeness_score"),
                        coherence_score=evaluation_data.get("coherence_score"),
                        helpfulness_score=evaluation_data.get("helpfulness_score"),
                        query_type=st.session_state.get("query_type"),
                        domain=st.session_state.get("domain"),
                        **kb_data,
                        **langchain_data,
                        **token_data,
                        **response_time_data,
                        feedback_comment=evaluation_data.get("feedback_comment"),
                        evaluation_source="streamlit"
                    )
                    evaluation_data = augmented_eval
                except Exception as e:
                    st.error(f"í‰ê°€ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # ëª¨ë¸ í†µê³„ ì—…ë°ì´íŠ¸
            model_base = model_name.split('-v')[0]
            
            if "evaluation_stats" not in st.session_state:
                st.session_state.evaluation_stats = {}
                
            if model_base not in st.session_state.evaluation_stats:
                st.session_state.evaluation_stats[model_base] = {
                    "count": 0,
                    "overall_sum": 0,
                    "accuracy_sum": 0,
                    "overall_avg": 0,
                    "accuracy_avg": 0
                }
                
            stats = st.session_state.evaluation_stats[model_base]
            stats["count"] += 1
            stats["overall_sum"] += evaluation_data["overall_score"]
            
            if "accuracy_score" in evaluation_data:
                stats["accuracy_sum"] += evaluation_data["accuracy_score"]
                
            stats["overall_avg"] = stats["overall_sum"] / stats["count"]
            
            if stats["count"] > 0:
                stats["accuracy_avg"] = stats["accuracy_sum"] / stats["count"]
            
            # ì œì¶œëœ í‰ê°€ ë°ì´í„° í‘œì‹œ
            st.subheader("ì œì¶œëœ í‰ê°€ ë°ì´í„°")
            
            # í•µì‹¬ ë©”íŠ¸ë¦­ í‘œì‹œ
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì „ì²´ ë§Œì¡±ë„", f"{evaluation_data['overall_score']}/10")
            
            if "relevance_score" in evaluation_data:
                with col2:
                    st.metric("ê´€ë ¨ì„±", f"{evaluation_data['relevance_score']}/10")
            
            if "accuracy_score" in evaluation_data:
                with col3:
                    st.metric("ì •í™•ì„±", f"{evaluation_data['accuracy_score']}/10")
            
            # ì‚¬ìš©ëœ ê¸°ìˆ  í‘œì‹œ
            tech_cols = st.columns(3)
            with tech_cols[0]:
                if "kb_used_in_query" in evaluation_data and evaluation_data["kb_used_in_query"]:
                    st.success("Bedrock ì§€ì‹ ê¸°ë°˜ ì‚¬ìš©ë¨")
                    if "kb_name" in evaluation_data and evaluation_data["kb_name"]:
                        st.caption(f"KB: {evaluation_data['kb_name']}")
            
            with tech_cols[1]:
                if "langchain_used" in evaluation_data and evaluation_data["langchain_used"]:
                    st.success("LangChain ì‚¬ìš©ë¨")
                    if "langchain_version" in evaluation_data and evaluation_data["langchain_version"]:
                        st.caption(f"ë²„ì „: {evaluation_data['langchain_version']}")
            
            with tech_cols[2]:
                if "total_tokens" in evaluation_data and evaluation_data["total_tokens"]:
                    st.info(f"ì´ í† í°: {evaluation_data['total_tokens']}")
                    prompt_tokens = evaluation_data.get("prompt_tokens", 0)
                    completion_tokens = evaluation_data.get("completion_tokens", 0)
                    st.caption(f"ì…ë ¥: {prompt_tokens}, ì¶œë ¥: {completion_tokens}")
            
            # ëª¨ë¸ í‰ê°€ ê¸°ë¡ í‘œì‹œ
            if "feedback_comment" in evaluation_data and evaluation_data["feedback_comment"]:
                st.info(f"í”¼ë“œë°±: {evaluation_data['feedback_comment']}")
            
            # ìƒì„¸ ë°ì´í„°(ì˜µì…˜)
            with st.expander("ëª¨ë“  í‰ê°€ ë°ì´í„°", expanded=False):
                st.json(evaluation_data)

if __name__ == "__main__":
    main() 